{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a7dbb5f",
   "metadata": {},
   "source": [
    "# Assignment 3 Unconstrained Nonlinear Numerical Optimization: Gradient Free and Approximation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ff9e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<p>\n",
    "A) Consider an objective function to be minimized:\n",
    "</p>\n",
    "\n",
    "<p style=\"margin-left:20px;\">\n",
    "f(x) = Σ<sub>k=1</sub><sup>2</sup> [ (-1)<sup>k</sup> (x<sub>k</sub> − k)<sup>2k</sup> ]\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "with x<sub>k</sub> ∈ [−5, 5]; k = 1, 2.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Consider Nelder–Mead’s Downhill Simplex Algorithm with parameters:\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "  <li>α = 1</li>\n",
    "  <li>β = 2</li>\n",
    "  <li>γ = 1/2</li>\n",
    "  <li>δ = 1/2</li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "Construct an initial simplex such that:\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "  <li>x<sub>1</sub> = [−5, 5]<sup>T</sup></li>\n",
    "  <li>x<sub>2</sub> = [0, 0]<sup>T</sup></li>\n",
    "  <li>x<sub>3</sub> = [5, −5]<sup>T</sup></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac78099",
   "metadata": {},
   "source": [
    "##### For 3 Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ce0ca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result after iterations:\n",
      "x: [-0.625  0.625]\n",
      "fun: 0.933837890625\n",
      "nit (iterations): 3\n",
      "message: Maximum number of iterations has been exceeded.\n",
      "success: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_18120\\2058112449.py:22: RuntimeWarning: Maximum number of iterations has been exceeded.\n",
      "  res = minimize(f_vec, x0=x2_init, method='Nelder-Mead',\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, lambdify\n",
    "from math import isfinite\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define symbolic variables and function: f(x) = sum_{k=1}^2 (-1)^k * (x_k - k)^(2k)\n",
    "x1, x2 = symbols('x1 x2')\n",
    "f_sym = (-1)**1 * (x1 - 1)**2 + (-1)**2 * (x2 - 2)**4  # simplifies to -(x1-1)^2 + (x2-2)^4\n",
    "f_num = lambdify((x1, x2), f_sym, 'math')  # numeric function using Python's math\n",
    "\n",
    "def f_vec(x):\n",
    "    # x is an array-like of length 2\n",
    "    return float(f_num(x[0], x[1]))\n",
    "\n",
    "# Given initial simplex vertices\n",
    "x1_init = [-5.0,  5.0]\n",
    "x2_init = [ 0.0,  0.0]\n",
    "x3_init = [ 5.0, -5.0]\n",
    "\n",
    "initial_simplex = [x1_init, x2_init, x3_init]\n",
    "\n",
    "# Use SciPy's minimize with Nelder-Mead\n",
    "res = minimize(f_vec, x0=x2_init, method='Nelder-Mead',\n",
    "               options={\n",
    "                   'initial_simplex': initial_simplex,\n",
    "                   'maxiter': 3,\n",
    "                   'xatol': 1e-12,\n",
    "                   'fatol': 1e-12,\n",
    "                   'disp': True\n",
    "               })\n",
    "\n",
    "print(\"Result after iterations:\")\n",
    "print(\"x:\", res.x)\n",
    "print(\"fun:\", res.fun)\n",
    "print(\"nit (iterations):\", res.nit)\n",
    "print(\"message:\", res.message)\n",
    "print(\"success:\", res.success)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4e359",
   "metadata": {},
   "source": [
    "##### For 5 Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0df3b2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result after iterations:\n",
      "x: [-3.28125  3.28125]\n",
      "fun: -15.634245872497559\n",
      "nit (iterations): 5\n",
      "message: Maximum number of iterations has been exceeded.\n",
      "success: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_18120\\4038032105.py:22: RuntimeWarning: Maximum number of iterations has been exceeded.\n",
      "  res = minimize(f_vec, x0=x2_init, method='Nelder-Mead',\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, lambdify\n",
    "from math import isfinite\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define symbolic variables and function: f(x) = sum_{k=1}^2 (-1)^k * (x_k - k)^(2k)\n",
    "x1, x2 = symbols('x1 x2')\n",
    "f_sym = (-1)**1 * (x1 - 1)**2 + (-1)**2 * (x2 - 2)**4  # simplifies to -(x1-1)^2 + (x2-2)^4\n",
    "f_num = lambdify((x1, x2), f_sym, 'math')  # numeric function using Python's math\n",
    "\n",
    "def f_vec(x):\n",
    "    # x is an array-like of length 2\n",
    "    return float(f_num(x[0], x[1]))\n",
    "\n",
    "# Given initial simplex vertices\n",
    "x1_init = [-5.0,  5.0]\n",
    "x2_init = [ 0.0,  0.0]\n",
    "x3_init = [ 5.0, -5.0]\n",
    "\n",
    "initial_simplex = [x1_init, x2_init, x3_init]\n",
    "\n",
    "# Use SciPy's minimize with Nelder-Mead\n",
    "res = minimize(f_vec, x0=x2_init, method='Nelder-Mead',\n",
    "               options={\n",
    "                   'initial_simplex': initial_simplex,\n",
    "                   'maxiter': 5,\n",
    "                   'xatol': 1e-12,\n",
    "                   'fatol': 1e-12,\n",
    "                   'disp': True\n",
    "               })\n",
    "\n",
    "print(\"Result after iterations:\")\n",
    "print(\"x:\", res.x)\n",
    "print(\"fun:\", res.fun)\n",
    "print(\"nit (iterations):\", res.nit)\n",
    "print(\"message:\", res.message)\n",
    "print(\"success:\", res.success)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3325dd",
   "metadata": {},
   "source": [
    "##### For 100 Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6de791c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -15.634872\n",
      "         Iterations: 64\n",
      "         Function evaluations: 138\n",
      "Result after iterations:\n",
      "x: [-3.28962389  3.28962389]\n",
      "fun: -15.634872460323553\n",
      "nit (iterations): 64\n",
      "message: Optimization terminated successfully.\n",
      "success: True\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, lambdify\n",
    "from math import isfinite\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define symbolic variables and function: f(x) = sum_{k=1}^2 (-1)^k * (x_k - k)^(2k)\n",
    "x1, x2 = symbols('x1 x2')\n",
    "f_sym = (-1)**1 * (x1 - 1)**2 + (-1)**2 * (x2 - 2)**4  # simplifies to -(x1-1)^2 + (x2-2)^4\n",
    "f_num = lambdify((x1, x2), f_sym, 'math')  # numeric function using Python's math\n",
    "\n",
    "def f_vec(x):\n",
    "    # x is an array-like of length 2\n",
    "    return float(f_num(x[0], x[1]))\n",
    "\n",
    "# Given initial simplex vertices\n",
    "x1_init = [-5.0,  5.0]\n",
    "x2_init = [ 0.0,  0.0]\n",
    "x3_init = [ 5.0, -5.0]\n",
    "\n",
    "initial_simplex = [x1_init, x2_init, x3_init]\n",
    "\n",
    "# Use SciPy's minimize with Nelder-Mead\n",
    "res = minimize(f_vec, x0=x2_init, method='Nelder-Mead',\n",
    "               options={\n",
    "                   'initial_simplex': initial_simplex,\n",
    "                   'maxiter': 100,\n",
    "                   'xatol': 1e-12,\n",
    "                   'fatol': 1e-12,\n",
    "                   'disp': True\n",
    "               })\n",
    "\n",
    "print(\"Result after iterations:\")\n",
    "print(\"x:\", res.x)\n",
    "print(\"fun:\", res.fun)\n",
    "print(\"nit (iterations):\", res.nit)\n",
    "print(\"message:\", res.message)\n",
    "print(\"success:\", res.success)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487ade2e",
   "metadata": {},
   "source": [
    "<p>\n",
    "B) Find the global minimum of Eason's function using the BFGS method.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-family:serif; margin-left:20px;\">\n",
    "  f(x) = cos(x) · e<sup>−(x − π)<sup>2</sup></sup>, &nbsp; x ∈ [−5, 5]\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  Investigate the effect of starting from different initial solutions, \n",
    "  say <span style=\"font-family:serif;\">x<sub>0</sub> = −5</span> and \n",
    "  <span style=\"font-family:serif;\">x<sub>0</sub> = 5</span>.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b698104",
   "metadata": {},
   "source": [
    "##### For 3 Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "745ac9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 0\n",
      "         Function evaluations: 2\n",
      "         Gradient evaluations: 1\n",
      "Start -5 → [-5.] 4.6276566469505575e-30\n",
      "         Current function value: 0.000001\n",
      "         Iterations: 3\n",
      "         Function evaluations: 18\n",
      "         Gradient evaluations: 9\n",
      "Start 5 → [6.82085293] 1.1348018149256301e-06\n"
     ]
    }
   ],
   "source": [
    "from math import pi, cos, exp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define Easom-like function\n",
    "def easom(x):\n",
    "    x = x[0]  # SciPy passes x as an array\n",
    "    return cos(x) * exp(-(x - pi)**2)\n",
    "\n",
    "# Run BFGS starting from x0 = -5\n",
    "res1 = minimize(easom, x0=[-5.0], method=\"BFGS\", options={\"disp\": True,\"maxiter\":3}) # Added maxiter to limit iterations to 3\n",
    "print(\"Start -5 →\", res1.x, res1.fun)\n",
    "\n",
    "# Run BFGS starting from x0 = 5\n",
    "res2 = minimize(easom, x0=[5.0], method=\"BFGS\", options={\"disp\": True,\"maxiter\":3}) # Added maxiter to limit iterations to 3\n",
    "print(\"Start 5 →\", res2.x, res2.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8f691",
   "metadata": {},
   "source": [
    "##### Unconstrained problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 0\n",
      "         Function evaluations: 2\n",
      "         Gradient evaluations: 1\n",
      "Start -5 → [-5.] 4.6276566469505575e-30\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000001\n",
      "         Iterations: 3\n",
      "         Function evaluations: 18\n",
      "         Gradient evaluations: 9\n",
      "Start 5 → [6.82085293] 1.1348018149256301e-06\n"
     ]
    }
   ],
   "source": [
    "from math import pi, cos, exp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define Easom-like function\n",
    "def easom(x):\n",
    "    x = x[0]  # SciPy passes x as an array\n",
    "    return cos(x) * exp(-(x - pi)**2)\n",
    "\n",
    "# Run BFGS starting from x0 = -5\n",
    "res1 = minimize(easom, x0=[-5.0], method=\"BFGS\", options={\"disp\": True})\n",
    "print(\"Start -5 →\", res1.x, res1.fun)\n",
    "\n",
    "# Run BFGS starting from x0 = 5\n",
    "res2 = minimize(easom, x0=[5.0], method=\"BFGS\", options={\"disp\": True})\n",
    "print(\"Start 5 →\", res2.x, res2.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25443f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<p>\n",
    "  The global minimum <span style=\"font-family:serif;\">f<sub>*</sub></span> = −1 occurs at \n",
    "  <span style=\"font-family:serif;\">x<sub>*</sub> = π</span>. \n",
    "  You may notice, starting from \n",
    "  <span style=\"font-family:serif;\">x<sub>0</sub> = 5</span> may obtain your optimal solution much quicker \n",
    "  than starting from <span style=\"font-family:serif;\">x<sub>0</sub> = −5</span>.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cda504",
   "metadata": {},
   "source": [
    "<p>\n",
    "C) Consider an objective function to be minimized:\n",
    "</p>\n",
    "\n",
    "<p style=\"margin-left:20px;\">\n",
    "f(x) = Σ<sub>k=1</sub><sup>2</sup> [ (-1)<sup>k</sup> (x<sub>k</sub> − k)<sup>2k</sup> ]\n",
    "</p>\n",
    "<p>\n",
    "  Apply the Trust Region Algorithm\n",
    "  Let the initial point be <span style=\"font-family:serif;\">[0, 0]<sup>T</sup></span> \n",
    "  and an initial trust region radius of 1.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  Algorithm parameters:\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "  <li><span style=\"font-family:serif;\">α<sub>1</sub> = 0.01</span></li>\n",
    "  <li><span style=\"font-family:serif;\">α<sub>2</sub> = 0.9</span></li>\n",
    "  <li><span style=\"font-family:serif;\">β<sub>1</sub> = β<sub>2</sub> = 0.5</span></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e538072",
   "metadata": {},
   "source": [
    "##### For 3 Iteration, uses Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c1599ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| niter |f evals|CG iter|  obj func   |tr radius |   opt    |  c viol  | penalty  |CG stop|\n",
      "|-------|-------|-------|-------------|----------|----------|----------|----------|-------|\n",
      "|   1   |   1   |   0   | +1.5000e+01 | 1.00e+00 | 3.20e+01 | 0.00e+00 | 1.00e+00 |   0   |\n",
      "|   2   |   2   |   2   | +8.5679e-02 | 7.00e+00 | 9.39e+00 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|   3   |   3   |   4   | -7.5597e+01 | 4.90e+01 | 1.74e+01 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "\n",
      "The maximum number of function evaluations is exceeded.\n",
      "Number of iterations: 3, function evaluations: 3, CG iterations: 4, optimality: 1.74e+01, constraint violation: 0.00e+00, execution time: 0.024 s.\n",
      "\n",
      "Final result after 3 iterations:\n",
      "x: [-7.70122791  1.41890305]\n",
      "f(x): -75.59734370789705\n"
     ]
    }
   ],
   "source": [
    "from math import pi\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the function\n",
    "def f(x):\n",
    "    x1, x2 = x\n",
    "    return -(x1 - 1)**2 + (x2 - 2)**4\n",
    "\n",
    "# Gradient\n",
    "def grad(x):\n",
    "    x1, x2 = x\n",
    "    return np.array([-2*(x1 - 1), 4*(x2 - 2)**3])\n",
    "\n",
    "# Hessian\n",
    "def hess(x):\n",
    "    x1, x2 = x\n",
    "    return np.array([[-2, 0], [0, 12*(x2 - 2)**2]])\n",
    "\n",
    "# Initial point\n",
    "x0 = np.array([0.0, 0.0])\n",
    "\n",
    "# Trust Region Solve (trust-constr)\n",
    "res = minimize(f, x0, method='trust-constr',\n",
    "               jac=grad, hess=hess,\n",
    "               options={'initial_tr_radius': 1.0,\n",
    "                        'maxiter': 3,   # only 3 iterations\n",
    "                        'verbose': 3})\n",
    "\n",
    "print(\"\\nFinal result after 3 iterations:\")\n",
    "print(\"x:\", res.x)\n",
    "print(\"f(x):\", res.fun)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d303b70b",
   "metadata": {},
   "source": [
    "##### For 3 Iteration, Used Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "030d0ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| niter |f evals|CG iter|  obj func   |tr radius |   opt    |  c viol  | penalty  |CG stop|\n",
      "|-------|-------|-------|-------------|----------|----------|----------|----------|-------|\n",
      "|   1   |   1   |   0   | +1.5000e+01 | 1.00e+00 | 3.20e+01 | 0.00e+00 | 1.00e+00 |   0   |\n",
      "|   2   |   2   |   2   | +8.5679e-02 | 7.00e+00 | 9.39e+00 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|   3   |   3   |   4   | -7.5597e+01 | 4.90e+01 | 1.74e+01 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "\n",
      "The maximum number of function evaluations is exceeded.\n",
      "Number of iterations: 3, function evaluations: 3, CG iterations: 4, optimality: 1.74e+01, constraint violation: 0.00e+00, execution time: 0.086 s.\n",
      "\n",
      "Final result after 3 iterations:\n",
      "x: [-7.70122792  1.41890302]\n",
      "f(x): -75.59734373651207\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define Easom-like function (from part a)\n",
    "def f(x):\n",
    "    x1, x2 = x\n",
    "    return -(x1 - 1)**2 + (x2 - 2)**4\n",
    "\n",
    "# Wrap with JAX\n",
    "f_jax = lambda x: f(x)\n",
    "\n",
    "# Gradient and Hessian via JAX\n",
    "grad_f = jax.grad(lambda x1, x2: f_jax((x1, x2)), argnums=(0,1))\n",
    "hess_f = jax.hessian(lambda xy: f_jax((xy[0], xy[1])))\n",
    "\n",
    "def fun(x):\n",
    "    return float(f_jax(x))\n",
    "\n",
    "def grad(x):\n",
    "    g = grad_f(x[0], x[1])\n",
    "    return jnp.array(g, dtype=float)\n",
    "\n",
    "def hess(x):\n",
    "    H = hess_f(x)\n",
    "    return jnp.array(H, dtype=float)\n",
    "\n",
    "x0 = jnp.array([0.0, 0.0])\n",
    "\n",
    "res = minimize(fun, x0, method=\"trust-constr\",\n",
    "               jac=grad, hess=hess,\n",
    "               options={\"initial_tr_radius\": 1.0,\n",
    "                        \"maxiter\": 3, \"verbose\": 3})\n",
    "\n",
    "print(\"\\nFinal result after 3 iterations:\")\n",
    "print(\"x:\", res.x)\n",
    "print(\"f(x):\", res.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be80cee3",
   "metadata": {},
   "source": [
    "##### For 3 Iteration, uses Sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd8326f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| niter |f evals|CG iter|  obj func   |tr radius |   opt    |  c viol  | penalty  |CG stop|\n",
      "|-------|-------|-------|-------------|----------|----------|----------|----------|-------|\n",
      "|   1   |   1   |   0   | +1.5000e+01 | 1.00e+00 | 3.20e+01 | 0.00e+00 | 1.00e+00 |   0   |\n",
      "|   2   |   2   |   2   | +8.5679e-02 | 7.00e+00 | 9.39e+00 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|   3   |   3   |   4   | -7.5597e+01 | 4.90e+01 | 1.74e+01 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "\n",
      "The maximum number of function evaluations is exceeded.\n",
      "Number of iterations: 3, function evaluations: 3, CG iterations: 4, optimality: 1.74e+01, constraint violation: 0.00e+00, execution time: 0.011 s.\n",
      "\n",
      "Final result after 3 iterations:\n",
      "x: [-7.70122791  1.41890305]\n",
      "f(x): -75.59734370789705\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Symbols\n",
    "x1, x2 = sp.symbols(\"x1 x2\")\n",
    "f_expr = -(x1 - 1)**2 + (x2 - 2)**4\n",
    "\n",
    "# Lambdify\n",
    "f_fun = sp.lambdify((x1, x2), f_expr, \"math\")\n",
    "grad_fun = sp.lambdify((x1, x2), [sp.diff(f_expr, x1), sp.diff(f_expr, x2)], \"math\")\n",
    "hess_fun = sp.lambdify((x1, x2), [[sp.diff(f_expr, x1, x1), sp.diff(f_expr, x1, x2)],\n",
    "                                  [sp.diff(f_expr, x2, x1), sp.diff(f_expr, x2, x2)]], \"math\")\n",
    "\n",
    "def fun(x):\n",
    "    return f_fun(x[0], x[1])\n",
    "\n",
    "def grad(x):\n",
    "    return grad_fun(x[0], x[1])\n",
    "\n",
    "def hess(x):\n",
    "    return hess_fun(x[0], x[1])\n",
    "\n",
    "x0 = [0.0, 0.0]\n",
    "\n",
    "res = minimize(fun, x0, method=\"trust-constr\",\n",
    "               jac=grad, hess=hess,\n",
    "               options={\"initial_tr_radius\": 1.0,\n",
    "                        \"maxiter\": 3, \"verbose\": 3})\n",
    "\n",
    "print(\"\\nFinal result after 3 iterations:\")\n",
    "print(\"x:\", res.x)\n",
    "print(\"f(x):\", res.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "151b26ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| niter |f evals|CG iter|  obj func   |tr radius |   opt    |  c viol  | penalty  |CG stop|\n",
      "|-------|-------|-------|-------------|----------|----------|----------|----------|-------|\n",
      "|   1   |   1   |   0   | +1.5000e+01 | 1.00e+00 | 3.20e+01 | 0.00e+00 | 1.00e+00 |   0   |\n",
      "|   2   |   2   |   2   | +8.5679e-02 | 7.00e+00 | 9.39e+00 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|   3   |   3   |   4   | -7.5597e+01 | 4.90e+01 | 1.74e+01 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|   4   |   4   |   5   | -3.3167e+03 | 3.43e+02 | 1.15e+02 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|   5   |   5   |   6   | -3.3167e+03 | 3.43e+01 | 1.15e+02 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|   6   |   6   |   7   | -8.2462e+03 | 2.40e+02 | 1.83e+02 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|   7   |   7   |   9   | -1.0987e+05 | 1.68e+03 | 6.63e+02 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|   8   |   8   |  11   | -1.0987e+05 | 1.68e+02 | 6.63e+02 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|   9   |   9   |  12   | -1.0987e+05 | 6.73e+01 | 6.63e+02 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  10   |  10   |  13   | -1.5525e+05 | 4.71e+02 | 1.72e+03 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  11   |  11   |  15   | -7.5572e+05 | 3.30e+03 | 1.74e+03 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  12   |  12   |  17   | -7.5572e+05 | 3.30e+02 | 1.74e+03 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  13   |  13   |  18   | -7.5572e+05 | 7.97e+01 | 1.74e+03 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  14   |  14   |  19   | -8.9783e+05 | 5.58e+02 | 1.90e+03 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  15   |  15   |  21   | -2.2679e+06 | 3.91e+03 | 3.01e+03 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  16   |  16   |  23   | -2.2679e+06 | 1.28e+03 | 3.01e+03 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  17   |  17   |  25   | -3.0749e+06 | 1.28e+03 | 4.04e+05 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  18   |  18   |  27   | -1.5646e+07 | 8.98e+03 | 1.20e+05 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  19   |  19   |  29   | -1.7016e+08 | 6.29e+04 | 3.46e+04 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  20   |  20   |  31   | -5.7630e+09 | 4.40e+05 | 1.52e+05 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  21   |  21   |  32   | -5.7630e+09 | 4.40e+04 | 1.52e+05 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  22   |  22   |  33   | -5.7630e+09 | 4.40e+03 | 1.52e+05 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  23   |  23   |  34   | -6.4217e+09 | 3.08e+04 | 1.57e+06 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  24   |  24   |  36   | -1.2342e+10 | 2.16e+05 | 4.54e+05 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  25   |  25   |  38   | -1.0676e+11 | 1.51e+06 | 6.53e+05 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  26   |  26   |  40   | -1.0676e+11 | 1.51e+05 | 6.53e+05 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  27   |  27   |  42   | -1.0676e+11 | 1.51e+04 | 6.53e+05 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  28   |  28   |  43   | -1.0676e+11 | 1.51e+03 | 6.53e+05 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  29   |  29   |  44   | -1.0773e+11 | 1.06e+04 | 1.03e+06 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  30   |  30   |  46   | -1.1479e+11 | 7.40e+04 | 6.78e+05 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  31   |  31   |  48   | -1.6597e+11 | 5.18e+05 | 6.85e+07 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  32   |  32   |  50   | -8.6491e+11 | 3.62e+06 | 2.03e+07 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  33   |  33   |  52   | -2.0743e+13 | 2.54e+07 | 9.11e+06 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  34   |  34   |  54   | -8.9523e+14 | 1.78e+08 | 7.23e+08 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  35   |  35   |  56   | -4.3054e+16 | 1.24e+09 | 4.15e+08 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  36   |  36   |  58   | -2.1038e+18 | 8.70e+09 | 1.68e+11 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  37   |  37   |  60   | -1.0305e+20 | 6.09e+10 | 4.96e+10 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  38   |  38   |  62   | -5.0494e+21 | 4.26e+11 | 1.42e+11 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  39   |  39   |  64   | -5.0494e+21 | 4.26e+10 | 1.42e+11 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  40   |  40   |  66   | -5.0494e+21 | 4.26e+09 | 1.42e+11 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  41   |  41   |  68   | -5.0494e+21 | 4.26e+08 | 1.42e+11 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  42   |  42   |  69   | -5.0494e+21 | 4.26e+07 | 1.42e+11 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  43   |  43   |  70   | -5.0494e+21 | 4.26e+06 | 1.42e+11 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  44   |  44   |  71   | -5.0494e+21 | 2.13e+06 | 1.42e+11 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  45   |  45   |  72   | -5.0497e+21 | 4.26e+06 | 1.33e+13 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  46   |  46   |  74   | -5.0503e+21 | 2.98e+07 | 3.93e+12 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  47   |  47   |  76   | -5.0546e+21 | 2.09e+08 | 1.16e+12 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  48   |  48   |  78   | -5.0843e+21 | 1.46e+09 | 3.37e+11 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  49   |  49   |  80   | -5.2950e+21 | 1.02e+10 | 1.46e+11 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  50   |  50   |  82   | -6.8896e+21 | 7.17e+10 | 1.66e+11 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  51   |  51   |  84   | -2.3919e+22 | 5.02e+11 | 8.71e+13 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  52   |  52   |  86   | -4.3068e+23 | 3.51e+12 | 2.58e+13 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  53   |  53   |  88   | -1.7368e+25 | 2.46e+13 | 8.33e+12 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  54   |  54   |  90   | -8.2633e+26 | 1.72e+14 | 8.01e+13 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  55   |  55   |  92   | -4.0319e+28 | 1.20e+15 | 4.02e+14 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  56   |  56   |  94   | -7.3644e+28 | 1.20e+15 | 2.05e+23 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  57   |  57   |  96   | -1.9921e+30 | 3.24e+15 | 1.71e+23 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  58   |  58   |  98   | -2.5946e+31 | 2.27e+16 | 2.44e+22 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  59   |  59   |  100  | -7.7016e+32 | 1.59e+17 | 9.77e+22 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  60   |  60   |  102  | -3.4738e+34 | 1.11e+18 | 6.47e+22 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  61   |  61   |  104  | -1.6815e+36 | 7.77e+18 | 3.47e+19 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  62   |  62   |  106  | -8.2248e+37 | 5.44e+19 | 1.81e+19 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  63   |  63   |  108  | -4.0291e+39 | 3.81e+20 | 5.12e+21 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  64   |  64   |  110  | -1.9742e+41 | 2.67e+21 | 8.89e+20 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  65   |  65   |  112  | -9.6735e+42 | 1.87e+22 | 3.34e+23 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  66   |  66   |  114  | -4.7400e+44 | 1.31e+23 | 2.35e+24 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  67   |  67   |  116  | -2.3226e+46 | 9.14e+23 | 3.96e+27 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  68   |  68   |  118  | -2.3226e+46 | 3.64e+23 | 3.96e+27 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  69   |  69   |  120  | -1.9639e+47 | 7.27e+23 | 5.44e+35 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  70   |  70   |  122  | -2.5247e+47 | 7.27e+23 | 1.61e+35 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  71   |  71   |  124  | -2.6355e+47 | 7.27e+23 | 4.77e+34 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  72   |  72   |  126  | -2.6991e+47 | 7.27e+23 | 1.73e+33 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  73   |  73   |  128  | -2.6999e+47 | 7.27e+23 | 6.41e+31 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  74   |  74   |  130  | -2.6999e+47 | 7.27e+22 | 6.41e+31 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  75   |  74   |  132  | -2.6999e+47 | 7.27e+21 | 6.41e+31 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  76   |  74   |  134  | -2.6999e+47 | 7.27e+20 | 6.41e+31 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  77   |  74   |  136  | -2.6999e+47 | 7.27e+19 | 6.41e+31 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  78   |  75   |  138  | -2.6999e+47 | 3.64e+19 | 6.41e+31 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  79   |  76   |  140  | -2.7002e+47 | 7.27e+19 | 7.71e+32 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  80   |  77   |  142  | -2.7003e+47 | 7.27e+19 | 2.32e+32 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  81   |  78   |  144  | -2.7003e+47 | 7.27e+19 | 5.76e+31 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  82   |  79   |  146  | -2.7003e+47 | 7.27e+19 | 9.47e+26 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  83   |  80   |  148  | -2.7011e+47 | 5.09e+20 | 2.83e+26 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  84   |  81   |  150  | -2.7063e+47 | 3.56e+21 | 7.46e+25 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  85   |  82   |  152  | -2.7436e+47 | 2.49e+22 | 7.12e+24 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  86   |  83   |  154  | -3.0111e+47 | 1.75e+23 | 3.84e+24 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  87   |  84   |  156  | -5.2322e+47 | 1.22e+24 | 3.26e+25 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  88   |  85   |  158  | -3.7854e+48 | 8.56e+24 | 6.39e+28 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  89   |  86   |  160  | -1.1028e+50 | 5.99e+25 | 5.77e+34 | 0.00e+00 | 1.00e+00 |   3   |\n",
      "|  90   |  87   |  162  | -1.1028e+50 | 5.99e+25 | 1.71e+34 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  91   |  88   |  164  | -1.1028e+50 | 5.99e+25 | 6.33e+32 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  92   |  89   |  166  | -1.1028e+50 | 5.99e+24 | 6.33e+32 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  93   |  89   |  168  | -1.1028e+50 | 5.99e+23 | 6.33e+32 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  94   |  89   |  170  | -1.1028e+50 | 5.99e+22 | 6.33e+32 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  95   |  89   |  172  | -1.1028e+50 | 5.99e+21 | 6.33e+32 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  96   |  89   |  174  | -1.1028e+50 | 5.99e+20 | 6.33e+32 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  97   |  89   |  176  | -1.1028e+50 | 5.99e+19 | 6.33e+32 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  98   |  89   |  178  | -1.1028e+50 | 1.85e+19 | 6.33e+32 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "|  99   |  90   |  180  | -1.1028e+50 | 3.71e+19 | 6.98e+33 | 0.00e+00 | 1.00e+00 |   2   |\n",
      "|  100  |  91   |  182  | -1.1028e+50 | 3.71e+19 | 1.91e+33 | 0.00e+00 | 1.00e+00 |   1   |\n",
      "\n",
      "The maximum number of function evaluations is exceeded.\n",
      "Number of iterations: 100, function evaluations: 91, CG iterations: 182, optimality: 1.91e+33, constraint violation: 0.00e+00, execution time: 0.17 s.\n",
      "\n",
      "Final result after 100 iterations:\n",
      "x: [-1.05014601e+25  7.81761577e+10]\n",
      "f(x): -1.1028062730902755e+50\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Symbols\n",
    "x1, x2 = sp.symbols(\"x1 x2\")\n",
    "f_expr = -(x1 - 1)**2 + (x2 - 2)**4\n",
    "\n",
    "# Lambdify\n",
    "f_fun = sp.lambdify((x1, x2), f_expr, \"math\")\n",
    "grad_fun = sp.lambdify((x1, x2), [sp.diff(f_expr, x1), sp.diff(f_expr, x2)], \"math\")\n",
    "hess_fun = sp.lambdify((x1, x2), [[sp.diff(f_expr, x1, x1), sp.diff(f_expr, x1, x2)],\n",
    "                                  [sp.diff(f_expr, x2, x1), sp.diff(f_expr, x2, x2)]], \"math\")\n",
    "\n",
    "def fun(x):\n",
    "    return f_fun(x[0], x[1])\n",
    "\n",
    "def grad(x):\n",
    "    return grad_fun(x[0], x[1])\n",
    "\n",
    "def hess(x):\n",
    "    return hess_fun(x[0], x[1])\n",
    "\n",
    "x0 = [0.0, 0.0]\n",
    "\n",
    "res = minimize(fun, x0, method=\"trust-constr\",\n",
    "               jac=grad, hess=hess,tol=1e-2,\n",
    "               options={\"initial_tr_radius\": 1.0,\n",
    "                        'maxiter': 100,\n",
    "                        \"verbose\": 3,\n",
    "                        'gtol': 1e-2,         # Gradient norm tolerance\n",
    "                        'xtol': 1e-2,         # Step size tolerance\n",
    "                        'barrier_tol': 1e-2,  # Optional for constraints\n",
    "                        })\n",
    "\n",
    "print(\"\\nFinal result after 100 iterations:\")\n",
    "print(\"x:\", res.x)\n",
    "print(\"f(x):\", res.fun)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
